---
title: "Day 03 - Design of Experiments"
author: "Hughes"
student: "Craig Beuerlein"
date: "Sections A: Wednesday, September 4, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Packages we will use today, note two new pacakges `ggfortify` and `emmeans`.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(ggfortify)
library(emmeans)
```

# Difference between DOE and OS

In many situations a practitioner simply collects measurements on predictor and response variables as they naturally occur, without intervention from the data collector. Such data is called **observational data** or data derived from an **observational study**.  An example is the MKT Teaching Evaluations data.

In a **designed experiment**, a researcher manipulates one or more variables, while holding all other variables constant. Typically the values of the predictor variables are discrete (that is, a countably finite number of controlled values). The main advantage of well-designed experiments over observational studies is that we can establish cause and effect relationships between the predictors and response. One of the most important things to keep in mind with analyzing designed experiments is that **the structure of the experiment dictates how the analysis may proceed**. 

# Example: Paired $t$-test

20 mice received a dietary treatment during 3 months. We want to know whether the dietary treatment has an impact on the weight of the mice. To answer to this question, the weight of the 20 mice have been measured before and after the treatment.

* **Response variable**: change of mice weight (typically after - before)
* **Experimental Unit**: A mouse, or the mice
* **Factor**: Time, because the researcher chose the time (after=3 months after before)
Note: Diet is NOT the factor, because it is not a variable in this study, every mouse gets the dietary treatment.
* **Controls**: Pairing - controls for nuisance variation within each mouse.

See the textbook for details on implementing a Paired $t$-test in R.

# Example: One-Way ANOVA

A tire manufacturer is interested in investigating the handling properties for different tread patterns. Data were recorded on the stopping distances measured to the nearest foot for a standard sized car to come to a complete stop from a speed of 60 miles per hour. There are six measurements of the stopping distance for each of four different tread patterns labeled A, B, C and D. The same driver and car were used for all 24 measurements and, although not clear from the saved data file, the order of treatments were assigned at random.

**Source**: Ugarte, M. D., Militino, A. F., and Arnholt, A. T. (2008) Probability and Statistics with R. Chapman \& Hall/CRC.

```{r}
Tire <- read.csv("http://users.miamioh.edu/hughesmr/sta363/TireData.csv")
glimpse(Tire)
```

Answer the following questions about the experimental design.

* What are the **Experimental Units** in this study? 

**The car**

* What is the **factor** in this study?

**The different treads of the tire**

* How many **factor levels** are there?

**There are 4 factor levels**

* What are the **treatments**?

**The measurements of stopping distance for each type of tire**

* What other steps were taken to control for nuisance variables and unexplained variability?

**The same car was used for all experiments, the same driver was used, **


## EDA

As with all analyses, we begin with an exploratory data analysis. This will reveal some basic information regarding the data. Since we are interested in tire-level information, we `group_by` tire type and calculuate summary values.

```{r}
Tire %>% 
  group_by(tire) %>% 
  summarize(Mean=mean(StopDist), 
            SD=sd(StopDist),
            Var=var(StopDist), 
            N=n() ) %>% 
  kable()
```

Next, we wish to graphical explore the data. In the code chunk below, write code that will construct side-by-side boxplots of the stopping distance as a function of the tire type.

```{r}
ggplot(Tire) + 
  geom_boxplot(aes(x=tire, y=StopDist)) +
  labs(x="Tread Type", y="Stopping Distance") + 
  theme_bw()
```

Discuss the Pros and Cons of the above plot

* **Pro**: **Since all 4 tire types are displayed next to each other, one can more easily interpret how the stopping distances of tires compare to one another. For example, though tire A has a lower average, its maximum goes above the average of tire B.**
* **Con**: **It is harder to focus on individual data points for a particular type of tire.**

## The Analysis - ANOVA

To analysze this sort of data problem we will perform a One-Way Analysis of Variance (ANOVA)

### Ideas of ANOVA

We begin with some basic analysis that will also help us explain the concept of One-Way ANOVA. First consider the following overall summary values:

```{r, echo=FALSE, warning=FALSE}
kable(Tire %>% summarize(Mean=mean(StopDist), Var=var(StopDist), N=n(), SS=var(StopDist)*(n()-1)))
```

You will note we calculate an **SS** which corresponds to **sum of squares**. Mathematically it is

$$SS_{Total} = \sum_{i=1}^n (Y_i - \bar{Y})^2$$

for observations $Y_i, i=1,\ldots,n$. We call this the sum of squares *total* because it is the total sum of squares in the *entire* sample. You should note this equation is nothing more than $S^2$ from your Intro Statistics course, except it is missing the degrees of freedom $n-1$.

Ultimately, we are interested in comparing the four groups and we saw earlier it appears the type of tire may matter. So how does each tire treatment perform?

```{r, echo=FALSE}
kable(Tire %>% group_by(tire) %>% summarize(Mean=mean(StopDist), Var=var(StopDist), N=n(), SS=var(StopDist)*(n()-1) ))
```

Ultimately we are interested in statistically testing if the type of tire (tread type) influences the mean stopping distance. Statistically we test:

$$H_0: \mu_A = \mu_B = \mu_C = \mu_D$$
versus 
$$H_A: \mu_i \neq \mu_j, ~\textrm{for some}~~ i,j=A,B,C,D$$

If the null hypothesis were true we would expect each of the four mean values of stopping distances to be approximately equal, and they should reasonably match the overall mean.

A way to measure the difference in the means compared to the overall mean is to look at the sum of squares for each group mean; mathematically:
$$(379.6667-404.2083)^2 + (405.1667-404.2083)^2 + (421.6667-404.2083)^2 + (410.3333-404.2083)^2 = 5673.125$$

If the null hypothesis were true, the above quantity should be reasonably close to the value of **zero**. If the value greatly exceeds zero, then we could argue that one of the tire tread means is different enough and we would reject the null hypothesis. The above value is typically known as the **sum of squares model**, **sum of squares treatments**, or the **between groups sum of squares**, labeled $SS_{Treatments}$.

Now the value of 5673.125 seems awfully far away from zero, but we have not accounted for any variability in that measurement (think back to one sample $t$-test, in the numerator you have $\bar{x}-\mu_0$ but that difference is scaled based on the standard error, $s/\sqrt{n}$). We need to account for variability. To consider that, first note that the $SS_{Treatments}$ is essentially a measure of variance. The $SS_{Treatments}$ essentially measures how much variability in the $SS_{Total}$ is explained by the different treatments (between the groups). What is left is *unexplained*, or is still within the treatments. This can is determined from the residuals *within* each group (a residual for a single point is $x_i - \bar{x}$), which essentially measures how much error, or noise, is left after we modeled $x_i$ with $\bar{x}$. We can find the total amount of variability unexplained with:

$$SS_{Error} = \sum_{j=1}^{K}\sum_{i=1}^{n_k} (Y_{j,i}-\bar{Y}_{j})^2$$
for $K$ different groups each of size $n_k$. You'll note the inside summation is essentially the variance (lacking the degrees of freedom) of each group. We have that in a table above!
$$SS_{Error} = 2471.333 + 852.8333 + 747.3333 + 3027.3333 = 7098.833$$
Now, note the following:
$$5673.125 + 7098.833 = 12771.96$$
which corresponds to the sum of squares total!

### How does ANOVA work?

In the above example, we essentially decomposed the variance in the total sample (the sum of squares total) into two parts, the sum of squares between the groups (the model or treatments part) and the sum of squares within the groups (the error or residuals). If the null hypothesis is true we would expect $SS_{Treatment}\approx 0$, thus $SS_{Error}\approx SS_{Total}$. If the null hypothesis were not true, we would expect the different treatments to explain most of the variability and thus $SS_{Treatment} \approx SS_{Total}$ with $SS_{Error}\approx 0$.

So it is called Analysis of Variance (ANOVA) because we essentially are comparing variance estimates. The statistic we use is an $F$-statistics which is based on the Sum of Squares but also incorporates the degrees of freedom to make a proper variance. 

$$F = \frac{SS_{Treatment}/(K-1)}{SS_{Error}/(n_1+\ldots+n_k - K)} = \frac{MS_{Treatment}}{MS_{Error}}$$
where there are $K$ treatments and each treatment has $n_i$, $i=1,\ldots,K$ replicates.

It can be shown (in STA463 and STA466) that if the null hypothesis is true $MS_{Treatment}\approx MS_{Error}$. Thus if the null hypothesis is true, $F\approx 1$. If the alternative is true, $F>1$. 

## Performing ANOVA in R

Performing ANOVA in R is quite easy if the data has been processed correctly. The results are typically displayed in an ANOVA table but before we look at the output of the fit, we also need to check the underling assumptions for ANOVA (like we did last week with the two-sample $t$-test). First we perform the ANOVA In R.

```{r}
tire.anova <- aov(StopDist ~ tire, data=Tire)
```

That's it!  One line, using the same notation as the $t$-test, `response ~ predictor`. We are fitting a model! We are simply telling R to model the `StopDist` response variable as a function of the predictor variable `tire`. 

When performing ANOVA we make the following assumptions.

* Underlying noise terms are independent
* Noise terms have constant variance
* Noise terms are Normally distributed

As before, assessing independence must come from collection of the data (in this case, the design of the experiment). The others can be assessed graphically checking the residuals using the `autoplot()` feature in the `ggfortify` package. The residuals are essentially an estimate of the random error, or noise, terms.

```{r}
library(ggfortify)
autoplot(tire.anova)
```

* Constant variance - Look at "Residuals vs Fitted" and "Scale-Location" plots. Blue line should be fairly horizontal and not see any systematic patterns in the plotted points.
* Normality - Look at the "Normal Q-Q" plot. Points should reasonably match the plotted 45-degree line.

Overall, we see nothing too concerning in these plots. There could be some concern about the constant variance assumption but there is nothing systematic in the plot. 

## ANOVA Output

Since our assumptions check out, we can now perform statistical inference by looking at the ANOVA output.

```{r}
summary(tire.anova)
```

Here we see an $F=5.328$ statistic on 3 and 20 degrees of freedom, which is significantly different than the value of 1 due to the $p$-value=0.0073. So we have evidence to suggest the different tire treads influence the stopping distance. However, this $p$-value is only valid since the underlying assumptions of ANOVA are met.


## Follow-up Procedures

We begin some follow-up procedures by considering a plot of the data.

```{r}
ggplot(Tire) + 
  geom_boxplot(aes(x=tire, y=StopDist), col="gray60" ) +
  geom_jitter(aes(x=tire, y=StopDist), width=0.1 ) +
  labs(x="Tread Type", y="Stopping Distance") + 
  theme_bw()
```

By eyeball method, it sure looks like the stopping distance is smaller in tire tread group $A$.

What does `geom_jitter()` do?

**geom_jitter() adds a small wobble (left to right) to the dataset to seperate data points that could be too hard to read**


### How do we statistically test for differences

It seems intuitive that we could perform a $t$-test comparing tread group $A$ to group $B$, and then tread group $A$ to $C$, $A$ to $D$, $B$ to $C$, $B$ to $D$ and $C$ to $D$. Note that One-Way ANOVA essentially is comprised of 6 two-sample comparisons! That is, it jointly makes all six comparisons at one time.

So why not just do six two-sample $t$-test?

The answer relates to probability. First recall from Intro Stats

$$\textrm{significance level} = \alpha = P(\textrm{Type I error})$$

Further,

$$P(\textrm{No Type I error}) = 1 - P(\textrm{Type I error}) = 1 - \alpha$$

by the complements rule. Now imagine I perform 2 statistical tests each at significance level $\alpha$. 

$$P(\textrm{No Type I errors in either test}) = P(\textrm{No Type I error in test 1 AND No Type I error in test 2})$$

The right hand side is comprised of two independent events (performing the first hypothesis test followed by a second). Thus

$$P(\textrm{No Type I errors in either test}) = (1 - \alpha)\times(1-\alpha) = (1-\alpha)^2$$

Suppose $\alpha=0.05$, then $P(\textrm{No Type I errors in either test}) = 0.95^2 = 0.9025$, thus $P(\textrm{Type I error occurs}) = 1 - 0.9025 = 0.0975$. 

By the same rationale, if we performed 6 hypothesis test, the probability of a Type I error occuring is 
$$1 - (1 - \alpha)^6 = 1 - 0.95^6 = 0.2649$$

So if we performed six two-sample *t*-test each at 5\% significance, there is greater than a 25\% chance we commit a Type I error somewhere in the multiple analysis! This is generally considered unacceptable. Think about as to why?

### Multiple Comparisons

There are several methods available to adjust the overall significance level when performing multiple hypothesis test (known as multiple comparisons). 

#### Bonferoni Correction

One of the simplest methods is to just adjust the $\alpha$-level for each test you perform, this is typically known as a Bonferroni correction. Basically, if you are performing $m$ hypothesis tests, or building $m$ confidence intervals, perform each with significance level
$$\alpha^* = \frac{\alpha}{m},$$
or with corresponding confidence level $1-\alpha^* = 1-\alpha/m$. This is guaranteed to control the overall Type I error rate to be less than $\alpha$. 

As an example consider performing two test each at $5\%$ significance level, we know the overall Type I error rate is closer to 9\% (see above). If each test was performed at $0.05/2=0.025$ we would get an overall Type I error rate of 
$$P(\textrm{Type I error is committed}) = 1-P(\textrm{No Type I error}) = 1 - (1-0.05/2)^2 = 0.049375 < 0.05$$ 

With six hypothesis test, we would perform each at $0.05/6 = 0.008333$ which results in an overall error rate of $0.04897 < 0.05$.

The Bonferroni method is always available but generally not preferred since the overall Type I error rate is less than acceptable.

### Tukey Honest Significant Differences

This method is attributed to John Tukey. The method controls the overall Type I error rate by adjusting the significance level each of the individual comparison but it does so in a more complex way than the naive Bonferroni approach above. The result can also be displayed as confidence intervals. It is implemented in the `emmeans` function.

```{r}
library(emmeans)                        # Load the package
tire.mc <- emmeans(tire.anova, "tire")  # Run emmeans on the factor "tire"
contrast(tire.mc, "pairwise")           # Perform pairwise comparisons
```

Here we see pairwise comparisons and the $p$-values of each comparison has been adjusted for the fact we are performing multiple comparisons. We see that group $A$ is different than $C$ and $D$, otherwise there is no difference in groups.

We can calculate confidence intervals of the comparisons.

```{r}
confint(contrast(tire.mc, "pairwise"))
```

We can also plot the pairwise comparisons

```{r}
plot(contrast(tire.mc, "pairwise"))
```

**Remember** when looking at the confidence intervals we are comparing to the value of **zero**, not the different intervals. 

### Dunnett's Comparison

Another method for comparison is known as Dunnett's multiple comparison. It works in a similar way (controlling the overall error rate) as Tukey. However, here one of the treatments (by default the first one listed) is considered a **control**. So all the comparisons are made against the control. Suppose tire tread $D$ is the *control*, we simply need to tell `emmeans` that the reference factor level (`ref`) is the fourth one list; i.e., treatment $D$.

```{r}
contrast(tire.mc, "trt.vs.ctrl", ref=4)
```

As before, we can plot and calculate confidence intervals.

```{r}
confint(contrast(tire.mc, "trt.vs.ctrl", ref=4))
plot(contrast(tire.mc, "trt.vs.ctrl", ref=4))
```

